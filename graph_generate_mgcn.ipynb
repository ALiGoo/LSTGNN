{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset_period = [datetime.strptime('2013-07-01', '%Y-%m-%d'), datetime.strptime('2017-10-01', '%Y-%m-%d')]\n",
    "test_period = [datetime.strptime('2017-10-01', '%Y-%m-%d') - timedelta(days=80), datetime.strptime('2017-10-01', '%Y-%m-%d')]\n",
    "valid_period = [test_period[0] - timedelta(days=40), test_period[0]]\n",
    "train_period = [dataset_period[0], valid_period[0]]\n",
    "predict_time = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_hdf('process_data/features_201307_201709.h5', key='features')\n",
    "station_info = pd.read_hdf('process_data/features_201307_201709.h5', key=\"info\")\n",
    "alive_df = pd.read_hdf('process_data/features_201307_201709.h5', key='alive')\n",
    "df_raw = pd.read_hdf('process_data/features_201307_201709.h5', key=\"raw\")\n",
    "df_raw['tripduration'] = (df_raw.stoptime - df_raw.starttime).dt.seconds\n",
    "df_raw.query('starttime >= @train_period[0] & stoptime < @train_period[1]', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_graph = (\n",
    "    station_info[[\"stationid\", \"lat\", \"lon\"]]\n",
    "    .assign(merge_key=1)\n",
    ")\n",
    "distance_graph = distance_graph.merge(distance_graph, on=\"merge_key\").drop(\n",
    "    \"merge_key\", axis=1\n",
    ")\n",
    "distance_graph[\"distance\"] = distance_graph.apply(\n",
    "    lambda x: geodesic((x.lat_x, x.lon_x), (x.lat_y, x.lon_y)).meters, axis=1\n",
    ")\n",
    "distance_graph = distance_graph.pivot(\n",
    "    index=\"stationid_x\", columns=\"stationid_y\", values=\"distance\"\n",
    ")\n",
    "distance_graph = distance_graph ** -1\n",
    "\n",
    "for i in range(len(distance_graph)):\n",
    "    distance_graph.iloc[i,i] = 0\n",
    "\n",
    "distance_graph = distance_graph.replace([np.inf, -np.inf], np.nan)\n",
    "distance_graph = distance_graph.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = (\n",
    "    df_raw[[\"startstationid\", \"endstationid\"]]\n",
    "    .groupby([\"startstationid\", \"endstationid\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"counts\")\n",
    ")\n",
    "interaction_graph = interaction_graph.pivot_table(\n",
    "    index=\"startstationid\", columns=\"endstationid\", values=\"counts\", fill_value=0\n",
    ")\n",
    "for i in range(len(interaction_graph)):\n",
    "    interaction_graph.iloc[i,i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_graph = df_raw[[\"endstationid\", \"startstationid\", \"stoptime\", \"starttime\"]]\n",
    "correlation_graph = correlation_graph.assign(\n",
    "    stoptime=correlation_graph.stoptime.dt.floor(predict_time),\n",
    "    starttime=correlation_graph.starttime.dt.floor(predict_time),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 3.75 s, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correlation_graph_in = (\n",
    "    correlation_graph.groupby([\"endstationid\", \"stoptime\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"flow_in\")\n",
    ")\n",
    "correlation_graph_in = correlation_graph_in.pivot_table(\n",
    "    index=\"stoptime\", columns=\"endstationid\", values=\"flow_in\", fill_value=0\n",
    ")\n",
    "correlation_graph_in = correlation_graph_in.corr(\"pearson\")\n",
    "correlation_graph_in[correlation_graph_in < 0] = 0\n",
    "\n",
    "for i in range(len(correlation_graph_in)):\n",
    "    correlation_graph_in.iloc[i,i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_norm(graph):\n",
    "    deg = np.array(np.sum(graph, axis=1))\n",
    "    deg = np.matrix(np.diag(deg)).astype(np.float32)\n",
    "    deg_inv = np.power(deg,-1)\n",
    "    deg_inv = np.where(np.isinf(deg_inv), 0, deg_inv)\n",
    "    A_norm = np.matmul(deg_inv, graph) + np.identity(graph.shape[0])\n",
    "    A_norm = torch.tensor(A_norm, dtype=torch.float32)\n",
    "    \n",
    "    return A_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_graph_norm = graph_norm(distance_graph.values)\n",
    "correlation_graph_in_norm = graph_norm(correlation_graph_in.values)\n",
    "interaction_graph_norm = graph_norm(interaction_graph.values)\n",
    "graph = torch.stack([distance_graph_norm, interaction_graph_norm, correlation_graph_in_norm], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training Validation Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_num = len(station_info)\n",
    "train_len = ((features.time >= train_period[0]) & (features.time < train_period[1])).sum() // station_num\n",
    "val_len = ((features.time >= valid_period[0]) & (features.time < valid_period[1])).sum() // station_num\n",
    "test_len = ((features.time >= test_period[0]) & (features.time < test_period[1])).sum() // station_num\n",
    "print(f\"train length:{train_len}\\nvalidation length:{val_len}\\ntest length:{test_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'is_weekend',\n",
       " 'stationid',\n",
       " 'y_in',\n",
       " 'y_out',\n",
       " 'bike_return_b1hour',\n",
       " 'bike_rent_b1hour',\n",
       " 'usertype_Customer_in',\n",
       " 'usertype_Subscriber_in',\n",
       " 'usertype_nan_in',\n",
       " 'gender_0.0_in',\n",
       " 'gender_2.0_in',\n",
       " 'gender_nan_in',\n",
       " 'usertype_Customer_out',\n",
       " 'usertype_Subscriber_out',\n",
       " 'usertype_nan_out',\n",
       " 'gender_0.0_out',\n",
       " 'gender_2.0_out',\n",
       " 'gender_nan_out',\n",
       " 'flow_in_b1hour',\n",
       " 'flow_out_b1hour',\n",
       " 'flow_in_b2hour',\n",
       " 'flow_out_b2hour',\n",
       " 'bike_return_b2hour',\n",
       " 'bike_rent_b2hour',\n",
       " 'flow_in_b1day',\n",
       " 'flow_out_b1day',\n",
       " 'bike_return_b1day',\n",
       " 'bike_rent_b1day',\n",
       " 'flow_in_b2day',\n",
       " 'flow_out_b2day',\n",
       " 'bike_return_b2day',\n",
       " 'bike_rent_b2day',\n",
       " 'flow_in_b3day',\n",
       " 'flow_out_b3day',\n",
       " 'bike_return_b3day',\n",
       " 'bike_rent_b3day',\n",
       " 'flow_in_b4day',\n",
       " 'flow_out_b4day',\n",
       " 'bike_return_b4day',\n",
       " 'bike_rent_b4day',\n",
       " 'flow_in_b1week',\n",
       " 'flow_out_b1week',\n",
       " 'bike_return_b1week',\n",
       " 'bike_rent_b1week',\n",
       " 'flow_in_b2week',\n",
       " 'flow_out_b2week',\n",
       " 'bike_return_b2week',\n",
       " 'bike_rent_b2week',\n",
       " 'month_2',\n",
       " 'month_3',\n",
       " 'month_4',\n",
       " 'month_5',\n",
       " 'month_6',\n",
       " 'month_7',\n",
       " 'month_8',\n",
       " 'month_9',\n",
       " 'month_10',\n",
       " 'month_11',\n",
       " 'month_12',\n",
       " 'dayofweek_1',\n",
       " 'dayofweek_2',\n",
       " 'dayofweek_3',\n",
       " 'dayofweek_4',\n",
       " 'dayofweek_5',\n",
       " 'dayofweek_6',\n",
       " 'hour_1',\n",
       " 'hour_2',\n",
       " 'hour_3',\n",
       " 'hour_4',\n",
       " 'hour_5',\n",
       " 'hour_6',\n",
       " 'hour_7',\n",
       " 'hour_8',\n",
       " 'hour_9',\n",
       " 'hour_10',\n",
       " 'hour_11',\n",
       " 'hour_12',\n",
       " 'hour_13',\n",
       " 'hour_14',\n",
       " 'hour_15',\n",
       " 'hour_16',\n",
       " 'hour_17',\n",
       " 'hour_18',\n",
       " 'hour_19',\n",
       " 'hour_20',\n",
       " 'hour_21',\n",
       " 'hour_22',\n",
       " 'hour_23',\n",
       " 'HourlyDryBulbTemperature',\n",
       " 'HourlyPrecipitation',\n",
       " 'HourlyRelativeHumidity',\n",
       " 'HourlyWindSpeed']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = features.columns\n",
    "columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flow_in_b1hour', 'flow_out_b1hour']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_column = ['flow_in_b1hour','flow_out_b1hour']\n",
    "print(len(gcn_column))\n",
    "gcn_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['is_weekend', 'usertype_Customer_in', 'usertype_Subscriber_in',\n",
       "       'usertype_nan_in', 'gender_0.0_in', 'gender_2.0_in', 'gender_nan_in',\n",
       "       'usertype_Customer_out', 'usertype_Subscriber_out', 'usertype_nan_out',\n",
       "       'gender_0.0_out', 'gender_2.0_out', 'gender_nan_out', 'month_2',\n",
       "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
       "       'month_9', 'month_10', 'month_11', 'month_12', 'dayofweek_1',\n",
       "       'dayofweek_2', 'dayofweek_3', 'dayofweek_4', 'dayofweek_5',\n",
       "       'dayofweek_6', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5',\n",
       "       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n",
       "       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n",
       "       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23',\n",
       "       'HourlyDryBulbTemperature', 'HourlyPrecipitation',\n",
       "       'HourlyRelativeHumidity', 'HourlyWindSpeed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_column = columns[~columns.str.contains('|'.join(['bike', 'flow', 'stationid', 'time', \"y_in\", \"y_out\"]))]\n",
    "print(len(fc_column))\n",
    "fc_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_in', 'y_out']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_column = [\"y_in\", \"y_out\"]\n",
    "y_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34056/34056 [01:40<00:00, 340.16it/s]\n"
     ]
    }
   ],
   "source": [
    "training_loader = []\n",
    "for i in tqdm(np.arange(1, train_len + 1, 1)):\n",
    "    x = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][gcn_column].values,\n",
    "        dtype=torch.float,\n",
    "    ).unsqueeze(0)\n",
    "    y = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][y_column].values,\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    x_fc = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][fc_column].values,\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    is_alive = torch.tensor(\n",
    "        alive_df.iloc[(i - 1) * station_num : i * station_num].is_alive.values,\n",
    "        dtype=torch.int8,\n",
    "    )\n",
    "    training_loader.append(\n",
    "        Data(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            x_fc=x_fc,\n",
    "            is_alive=is_alive,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [00:02<00:00, 350.93it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_loader = []\n",
    "for i in tqdm(np.arange(train_len + 1, train_len + val_len + 1, 1)):\n",
    "    x = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][gcn_column].values,\n",
    "        dtype=torch.float,\n",
    "    ).unsqueeze(0)\n",
    "    y = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][y_column].values,\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    x_fc = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][fc_column].values,\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    is_alive = torch.tensor(\n",
    "        alive_df.iloc[(i - 1) * station_num : i * station_num].is_alive.values,\n",
    "        dtype=torch.int8,\n",
    "    )\n",
    "    validation_loader.append(\n",
    "        Data(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            x_fc=x_fc,\n",
    "            is_alive=is_alive,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1920/1920 [00:05<00:00, 352.23it/s]\n"
     ]
    }
   ],
   "source": [
    "testing_loader = []\n",
    "for i in tqdm(\n",
    "    np.arange(train_len + val_len + 1, train_len + val_len + test_len + 1, 1)\n",
    "):\n",
    "    x = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][gcn_column].values,\n",
    "        dtype=torch.float,\n",
    "    ).unsqueeze(0)\n",
    "    \n",
    "    y = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][y_column].values,\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    x_fc = torch.tensor(\n",
    "        features.iloc[(i - 1) * station_num : i * station_num][fc_column].values,\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    is_alive = torch.tensor(\n",
    "        alive_df.iloc[(i - 1) * station_num : i * station_num].is_alive.values,\n",
    "        dtype=torch.int8,\n",
    "    )\n",
    "    testing_loader.append(\n",
    "        Data(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            x_fc=x_fc,\n",
    "            is_alive=is_alive,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process_data/loader_mgcn.pt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(\n",
    "    {\n",
    "        'training_loader':training_loader,\n",
    "        'validation_loader':validation_loader,\n",
    "        'testing_loader':testing_loader,\n",
    "        'graph':graph,\n",
    "    },\n",
    "    'process_data/loader_mgcn.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_bike",
   "language": "python",
   "name": "gnn_bike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
