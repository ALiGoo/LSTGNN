{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dataset_period = [datetime.strptime('2013-07-01', '%Y-%m-%d'), datetime.strptime('2017-10-01', '%Y-%m-%d')]\n",
    "test_period = [datetime.strptime('2017-10-01', '%Y-%m-%d') - timedelta(days=80), datetime.strptime('2017-10-01', '%Y-%m-%d')]\n",
    "valid_period = [test_period[0] - timedelta(days=40), test_period[0]]\n",
    "train_period = [dataset_period[0], valid_period[0]]\n",
    "predict_time = \"H\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_hdf(\"process_data_201306_201709/citibike_raw.h5\", key=\"raw\")\n",
    "station_info = pd.read_hdf(\"process_data_201306_201709/citibike_raw.h5\", key=\"info\")\n",
    "weather = pd.read_hdf(\"process_data_201306_201709/citibike_raw.h5\", key=\"weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info.query('earliest < @train_period[1]', inplace=True)\n",
    "df_raw.query('startstationid in @station_info.stationid & endstationid in @station_info.stationid', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.get_dummies(\n",
    "    df_raw, dummy_na=True, columns=[\"usertype\", \"gender\"], dtype=np.int8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_in = (\n",
    "    df_raw.assign(stoptime=df_raw.stoptime.dt.floor(predict_time))\n",
    "    .groupby([\"endstationid\", \"stoptime\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"flow_in\")\n",
    "    .rename(columns={\"stoptime\": \"time\", \"endstationid\": \"stationid\"})\n",
    ")\n",
    "\n",
    "flow_out = (\n",
    "    df_raw.assign(starttime=df_raw.starttime.dt.floor(predict_time))\n",
    "    .groupby([\"startstationid\", \"starttime\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"flow_out\")\n",
    "    .rename(columns={\"starttime\": \"time\", \"startstationid\": \"stationid\"})\n",
    ")\n",
    "\n",
    "bike_return = (\n",
    "    df_raw.assign(\n",
    "        stoptime=df_raw.stoptime.dt.ceil(predict_time),\n",
    "        starttime=df_raw.starttime.dt.ceil(predict_time),\n",
    "    )\n",
    "    .query(\"starttime != stoptime\")\n",
    "    .groupby([\"endstationid\", \"stoptime\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"bike_return\")\n",
    "    .rename(columns={\"stoptime\": \"time\", \"endstationid\": \"stationid\"})\n",
    ")\n",
    "\n",
    "bike_rent = (\n",
    "    df_raw.assign(\n",
    "        stoptime=df_raw.stoptime.dt.ceil(predict_time),\n",
    "        starttime=df_raw.starttime.dt.ceil(predict_time),\n",
    "    )\n",
    "    .query(\"starttime != stoptime\")\n",
    "    .groupby([\"startstationid\", \"starttime\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"bike_rent\")\n",
    "    .rename(columns={\"starttime\": \"time\", \"startstationid\": \"stationid\"})\n",
    ")\n",
    "\n",
    "category_in = (\n",
    "    df_raw.assign(stoptime=df_raw.stoptime.dt.ceil(predict_time))\n",
    "    .groupby([\"endstationid\", \"stoptime\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"usertype_Customer\": np.sum,\n",
    "            \"usertype_Subscriber\": np.sum,\n",
    "            \"usertype_nan\": np.sum,\n",
    "            \"gender_0.0\": np.sum,\n",
    "            \"gender_2.0\": np.sum,\n",
    "            \"gender_nan\": np.sum,\n",
    "        }\n",
    "    )\n",
    "    .add_suffix(\"_in\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"stoptime\": \"time\", \"endstationid\": \"stationid\"})\n",
    ")\n",
    "\n",
    "category_out = (\n",
    "    df_raw.assign(starttime=df_raw.starttime.dt.ceil(predict_time))\n",
    "    .groupby([\"startstationid\", \"starttime\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"usertype_Customer\": np.sum,\n",
    "            \"usertype_Subscriber\": np.sum,\n",
    "            \"usertype_nan\": np.sum,\n",
    "            \"gender_0.0\": np.sum,\n",
    "            \"gender_2.0\": np.sum,\n",
    "            \"gender_nan\": np.sum,\n",
    "        }\n",
    "    )\n",
    "    .add_suffix(\"_out\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"starttime\": \"time\", \"startstationid\": \"stationid\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.date_range(\n",
    "    datetime.strptime(\"2013-07-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\"),\n",
    "    datetime.strptime(\"2017-09-30 23:00:00\", \"%Y-%m-%d %H:%M:%S\"),\n",
    "    freq=predict_time,\n",
    ")\n",
    "features = list(product(features, station_info.stationid))\n",
    "features = pd.DataFrame(features, columns=[\"time\", \"stationid\"])\n",
    "features = features.merge(flow_in, on=[\"time\", \"stationid\"], how=\"left\")\n",
    "features = features.merge(flow_out, on=[\"time\", \"stationid\"], how=\"left\")\n",
    "features = features.merge(bike_return, on=[\"time\", \"stationid\"], how=\"left\")\n",
    "features = features.merge(bike_rent, on=[\"time\", \"stationid\"], how=\"left\")\n",
    "features = features.merge(category_in, on=[\"time\", \"stationid\"], how=\"left\")\n",
    "features = features.merge(category_out, on=[\"time\", \"stationid\"], how=\"left\")\n",
    "features.fillna(0, inplace=True)\n",
    "features[features.columns[1:]] = features[features.columns[1:]].astype(\"int16\")\n",
    "\n",
    "del flow_in, flow_out, bike_return, bike_rent, category_in, category_out\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (\n",
    "    features.assign(is_weekend=features.time.dt.dayofweek >= 5)\n",
    "    .astype({\"is_weekend\": \"int8\"})\n",
    "    .set_index([\"time\", \"is_weekend\", \"stationid\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shift_column = ['flow_in', 'flow_out', 'bike_return', 'bike_rent']\n",
    "features = pd.concat(\n",
    "    [\n",
    "        features.rename(columns={\"flow_in\": \"y_in\", \"flow_out\": \"y_out\", \"bike_return\":\"bike_return_b1hour\", \"bike_rent\":\"bike_rent_b1hour\"}),\n",
    "        features[[\"flow_in\", \"flow_out\"]].groupby(level=2).shift(1, fill_value=-1).add_suffix(\"_b1hour\"),\n",
    "        features[shift_column].groupby(level=2).shift(2, fill_value=-1).add_suffix(\"_b2hour\"),\n",
    "        features[shift_column].groupby(level=[1, 2]).shift(24, fill_value=-1).add_suffix(\"_b1day\"),\n",
    "        features[shift_column].groupby(level=[1, 2])\n",
    "        .shift(24 * 2, fill_value=-1)\n",
    "        .add_suffix(\"_b2day\"),\n",
    "        features[shift_column].groupby(level=[1, 2])\n",
    "        .shift(24 * 3, fill_value=-1)\n",
    "        .add_suffix(\"_b3day\"),\n",
    "        features[shift_column].groupby(level=[1, 2])\n",
    "        .shift(24 * 4, fill_value=-1)\n",
    "        .add_suffix(\"_b4day\"),\n",
    "        features[shift_column].groupby(level=2).shift(24 * 7, fill_value=-1).add_suffix(\"_b1week\"),\n",
    "        features[shift_column].groupby(level=2).shift(24 * 14, fill_value=-1).add_suffix(\"_b2week\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ").iloc[24 * 14 * len(station_info) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.reset_index()\n",
    "\n",
    "features = features.assign(\n",
    "    month=features.time.dt.month,\n",
    "    dayofweek=features.time.dt.dayofweek,\n",
    "    hour=features.time.dt.hour,\n",
    ")\n",
    "\n",
    "features = pd.get_dummies(\n",
    "    features, columns=[\"month\", \"dayofweek\", \"hour\"], drop_first=True, dtype=np.int8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score normalize\n",
    "norm_col = weather.columns[1:]\n",
    "weather[norm_col] = (weather[norm_col] - weather[norm_col].mean()) / weather[norm_col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.date_range(\n",
    "    datetime.strptime(\"2013-07-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\"),\n",
    "    datetime.strptime(\"2017-09-30 23:00:00\", \"%Y-%m-%d %H:%M:%S\"),\n",
    "    freq=predict_time,\n",
    ")\n",
    "time = pd.DataFrame(time,columns=['time'])\n",
    "weather = time.merge(weather, on=[\"time\"], how=\"left\")\n",
    "weather = weather.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat weather to features\n",
    "features = features.merge(weather, on=\"time\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alive Datafeame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alive_df = features[[\"stationid\", \"time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for stationid in tqdm(station_info.stationid):\n",
    "    condition = (\n",
    "        alive_df.loc[alive_df.stationid == stationid, \"time\"]\n",
    "        >= station_info.loc[station_info.stationid == stationid, \"earliest\"].values[0]\n",
    "    ) & (\n",
    "        alive_df.loc[alive_df.stationid == stationid, \"time\"]\n",
    "        <= station_info.loc[station_info.stationid == stationid, \"latest\"].values[0]\n",
    "    )\n",
    "    alive_df.loc[alive_df.stationid == stationid, \"is_alive\"] = np.where(\n",
    "        condition, 1, 0\n",
    "    )\n",
    "alive_df[\"is_alive\"] = alive_df[\"is_alive\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_hdf('process_data_201306_201709/features.h5', key=\"features\", mode=\"w\")\n",
    "df_raw.to_hdf('process_data_201306_201709/features.h5', key=\"raw\", mode=\"r+\")\n",
    "station_info.to_hdf('process_data_201306_201709/features.h5', key=\"info\", mode=\"r+\")\n",
    "alive_df.to_hdf('process_data_201306_201709/features.h5', key=\"alive\", mode=\"r+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_bike",
   "language": "python",
   "name": "gnn_bike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
